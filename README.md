# LlamaCppGradio
Run Llama Cpp with Python Bindings and Gradio UI

This code is meant to run multimodal Llama.cpp with python bindings and a Gradio user interface for simplicity locally

Create a virtual environment

In terminal, install requirements using text
"pip install -r requirements.txt"

In line 6 of python code, replace your model path with whatever model you are currently running
"model_path = "/EdgeRunner-Light-Q4_K_M.gguf"   # Replace with your model's path"

Replace "/edgerunner-light-q4_k_m.gguf" with whatever model it is that you are using (link with many llama models below provided)
https://huggingface.co/meta-llama
