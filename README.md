# LlamaCppGradio
Run Llama Cpp with Python Bindings and Gradio UI

This code is meant to run Llama.cpp with python bindings and a Gradio user interface for simplicity locally

Clone this repo into a virtual environment

In terminal, install requirements
"pip3 install -r requirements.txt"

In line 6 of the code, replace your model path with whatever model you are currently using
"model_path = "/Qwen2-VL-7B-Instruct-Q4_K_M.gguf"   # Replace with your model's path"

Replace </edgerunner-light-q4_k_m.gguf> with whatever model it is that you are using (link with many llama models below provided)
https://huggingface.co/meta-llama

Run the program and follow available open port for a localhost server
